{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from os import path\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import interp\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worker Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance measures between the predicted y_prob values and the ground truth y_test values\n",
    "# Based on default threshold value of 0.5 or given threshold in_thresh\n",
    "def getPerformanceMeasurements(y_test, y_prob, in_thresh):\n",
    "    if in_thresh == -1:\n",
    "        cm = confusion_matrix(y_test, y_prob >= 0.5)\n",
    "    else:\n",
    "        cm = confusion_matrix(y_test, y_prob >= in_thresh)\n",
    "\n",
    "    TP = cm[1][1]\n",
    "    FN = cm[1][0]\n",
    "    FP = cm[0][1]\n",
    "    TN = cm[0][0]\n",
    "\n",
    "    if (TP + FP) == 0 or (TP + FN) == 0 or ((TN + FP) == 0) or (TN + FN) == 0:\n",
    "        PR = 0\n",
    "        mcc = 0\n",
    "    else:\n",
    "        PR = TP / (TP + FP)\n",
    "        mcc = ((TP * TN) - (FP * FN)) / \\\n",
    "            (np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "\n",
    "    RE = TP / (TP + FN)\n",
    "    SP = TN / (TN + FP)\n",
    "    acc = (TP + TN) / (TP + FP + FN + TN)\n",
    "    F05 = getFScore(0.5, PR, RE)\n",
    "    F1 = getFScore(1, PR, RE)\n",
    "    F2 = getFScore(2, PR, RE)\n",
    "\n",
    "    return PR, RE, SP, F1, F2, acc, mcc, cm, TP, FP, TN, FN\n",
    "\n",
    "\n",
    "# Perform z-score normalization for a given athlete id\n",
    "def normalize2(row, mean_df, std_df, athlete_id):\n",
    "    mu = mean_df.loc[athlete_id]\n",
    "    su = std_df.loc[athlete_id]\n",
    "    z = (row - mu)/su\n",
    "    return z\n",
    "\n",
    "# Calculate the means and standard deviations of all healthy events per athlete\n",
    "def getMeanStd(data):\n",
    "    mean = data[data['injury'] == 0].groupby('Athlete ID').mean()\n",
    "    std = data[data['injury'] == 0].groupby('Athlete ID').std()\n",
    "    std.replace(to_replace=0.0, value=0.01, inplace=True)\n",
    "    return mean, std\n",
    "\n",
    "# Calculate the F-score for a given beta value\n",
    "def getFScore(beta, PR, RE):\n",
    "    if PR == 0 and RE == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (1+(beta*beta))*(PR*RE)/((beta*beta*PR) + RE)\n",
    "\n",
    "# Calculate the required statistical results\n",
    "def getStats(y_test, y_pred, y_prob, in_thresh, show):\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    fpr = np.linspace(0, 1, 101)\n",
    "    fpr_org, tpr_org, threshold = metrics.roc_curve(y_test, y_prob)\n",
    "    tpr = interp(fpr, fpr_org, tpr_org)\n",
    "\n",
    "    if not in_thresh:\n",
    "        n_thresh = len(threshold)\n",
    "        prlist = np.zeros(n_thresh)\n",
    "        relist = np.zeros(n_thresh)\n",
    "        acclist = np.zeros(n_thresh)\n",
    "        mcclist = np.zeros(n_thresh)\n",
    "        F2list = np.zeros(n_thresh)\n",
    "        F1list = np.zeros(n_thresh)\n",
    "        SPlist = np.zeros(n_thresh)\n",
    "        for th in np.arange(n_thresh):\n",
    "            prlist[th], relist[th], SPlist[th], F1list[th], F2list[th], acclist[th], mcclist[th], cm, TP, FP, TN, FN = getPerformanceMeasurements(\n",
    "                y_test, y_prob, threshold[th])\n",
    "\n",
    "        idx = np.argmin(np.abs(np.array(relist) - np.array(SPlist)))\n",
    "        best_test_thresh = threshold[idx]\n",
    "\n",
    "        PR, RE, SP, F1, F2, acc, mcc, cm, TP, FP, TN, FN = getPerformanceMeasurements(\n",
    "            y_test, y_prob, best_test_thresh)\n",
    "        stats = {\"thresh\": best_test_thresh, \"auc\": auc,\n",
    "                 \"fpr\": fpr, \"tpr\": tpr, \"cm\": cm}\n",
    "    else:\n",
    "        PR, RE, SP, F1, F2, acc, mcc, cm, TP, FP, TN, FN = getPerformanceMeasurements(\n",
    "            y_test, y_prob, in_thresh)\n",
    "        stats = {\"thresh\": in_thresh, \"auc\": auc,\n",
    "                 \"fpr\": fpr, \"tpr\": tpr, \"cm\": cm}\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Load the data set\n",
    "def loadData(approach):\n",
    "    if (approach == \"day\"):\n",
    "        df = pd.read_csv(\"day_approach_maskedID_timeseries.csv\")\n",
    "    elif (approach == \"week\"):\n",
    "        df = pd.read_csv(\"week_approach_maskedID_timeseries.csv\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost model with the given dftrain data and calibrate it with the given X_val data\n",
    "def trainModel(params, dftrain, X_val, mean, std, calibrate):\n",
    "    y_train = np.array(dftrain['injury']).astype(int)\n",
    "    y_val = np.array(X_val['injury']).astype(int)\n",
    "\n",
    "    data_train = dftrain.drop(columns=['injury', 'Date'])\n",
    "    data_val = X_val.drop(columns=['injury', 'Date'])\n",
    "\n",
    "    '''Apply normalization to training set'''\n",
    "    data_train = data_train.apply(lambda x: normalize2(\n",
    "        x, mean, std, x['Athlete ID']), axis=1)\n",
    "    data_val = data_val.apply(lambda x: normalize2(\n",
    "        x, mean, std, x['Athlete ID']), axis=1)\n",
    "\n",
    "    X_train = data_train.drop(\n",
    "        columns=['injury', 'Date', 'Athlete ID'], errors='ignore').to_numpy()\n",
    "    X_val = data_val.drop(\n",
    "        columns=['injury', 'Date', 'Athlete ID'], errors='ignore').to_numpy()\n",
    "\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', learning_rate=0.01,\n",
    "                              max_depth=random.choice(\n",
    "                                  params[\"XGBDepthList\"]),\n",
    "                              n_estimators=random.choice(\n",
    "                                  params[\"XGBEstimatorsList\"]),\n",
    "                              importance_type='total_gain', eval_metric='auc', verbosity=1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    calib_model = CalibratedClassifierCV(model, method=calibrate, cv=\"prefit\")\n",
    "    calib_model.fit(X_val, y_val)\n",
    "\n",
    "    return model, calib_model, X_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Balanced Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Balanced Subset\n",
    "def getBalancedSubset(X_train, samplesPerClass):\n",
    "    healthySet = pd.DataFrame\n",
    "    injuredSet = pd.DataFrame\n",
    "\n",
    "    # determine which athletes have injured and uninjured events\n",
    "    stats = pd.DataFrame(X_train[['Athlete ID', 'injury']].groupby(\n",
    "        ['Athlete ID', 'injury']).size().reset_index(name='counts'))  # group by athletes and injury/non-injury, and count of each\n",
    "    stats = pd.DataFrame(stats[['Athlete ID']].groupby(\n",
    "        ['Athlete ID']).size().reset_index(name='counts'))  # type: ignore # group by number athletes (and get size ie. are there injured and uninjured events )\n",
    "    stats.drop(stats[stats['counts'] < 2].index, inplace=True)\n",
    "    athleteList = stats['Athlete ID'].unique()\n",
    "\n",
    "    samplesPerAthlete = int(np.floor(samplesPerClass) / len(athleteList))\n",
    "\n",
    "    for athlete in athleteList:\n",
    "        if injuredSet.empty:\n",
    "            injuredSet = X_train[(X_train['Athlete ID'] == athlete) & (\n",
    "                X_train['injury'] == 0\n",
    "            )].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            dataToAdd = X_train[(X_train['Athlete ID'] == athlete) & (\n",
    "                X_train['injury'] == 0\n",
    "            )].sample(samplesPerAthlete, replace=True)\n",
    "            injuredSet = pd.concat([injuredSet, dataToAdd])\n",
    "\n",
    "        if healthySet.empty:\n",
    "            healthySet = X_train[(X_train['Athlete ID'] == athlete) & (\n",
    "                X_train['injury'] == 1\n",
    "            )].sample(samplesPerAthlete, replace=True)\n",
    "        else:\n",
    "            dataToAdd = X_train[(X_train['Athlete ID'] == athlete) & (\n",
    "                X_train['injury'] == 1\n",
    "            )].sample(samplesPerAthlete, replace=True)\n",
    "            healthySet = pd.concat([healthySet, dataToAdd])\n",
    "\n",
    "    balancedSet = pd.concat([healthySet, injuredSet])\n",
    "    return balancedSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the bagging approach to new test data\n",
    "def applyBagging(modelList, X_test, X_test_means, X_test_std, best_train_thresh, train_auc_list, filename):\n",
    "    # Test the model on the test set\n",
    "    y_predList = pd.DataFrame()\n",
    "    y_probList = pd.DataFrame()\n",
    "    fprList = pd.DataFrame()\n",
    "    tprList = pd.DataFrame()\n",
    "    auc_test_list = []\n",
    "\n",
    "    y_test = np.array(X_test['injury']).astype(int)\n",
    "    X_test = X_test.drop(columns=['injury', 'Date'])\n",
    "    X_test = X_test.apply(lambda x: normalize2(\n",
    "        x, X_test_means, X_test_std, x['Athlete ID']), axis=1)\n",
    "    X_test = X_test.drop(\n",
    "        columns=['injury', 'Date', 'Athlete ID'], errors='ignore').to_numpy()\n",
    "\n",
    "    for idx, model in enumerate(modelList):\n",
    "        y_pred, y_prob = predictValues(X_test, model)\n",
    "\n",
    "        indices = np.where(np.array(y_pred) != None)\n",
    "        y_test = np.array(y_test)[indices].astype(int)\n",
    "        y_pred = np.array(y_pred)[indices].astype(int)\n",
    "        y_prob = np.array(y_prob)[indices]\n",
    "        y_predList['bag %d' % (idx+1)] = y_pred\n",
    "        y_probList['bag %d' % (idx+1)] = y_prob\n",
    "\n",
    "        stats = getStats(y_test, y_pred, y_prob, 0.5, False)\n",
    "        fprList['bag %d' % (idx + 1)] = stats[\"fpr\"]\n",
    "        tprList['bag %d' % (idx + 1)] = stats[\"tpr\"]\n",
    "        auc_test_list.append(stats[\"auc\"])\n",
    "\n",
    "    y_prob = y_probList.mean(axis=1)\n",
    "    y_pred = y_predList.mode(axis=1)\n",
    "\n",
    "    plotCalibrationCurve(y_test, y_prob, filename)\n",
    "\n",
    "    val_stats = getStats(np.array(y_test).astype(int), np.array(\n",
    "        y_pred).astype(int), np.array(y_prob), best_train_thresh, True)\n",
    "    return val_stats[\"thresh\"], val_stats[\"auc\"], fprList.mean(axis=1), tprList.mean(axis=1), val_stats[\"cm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot a calibration with the given y_test and y_pred values\n",
    "def plotCalibrationCurve(y_test, y_pred, filename):\n",
    "    fig = plt.figure(1, figsize=(10, 10))\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test, y_pred, n_bins=10, strategy='quantile')\n",
    "\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"XGBoost\")\n",
    "    plt.plot([0, 1], [0, 1], ls=\"--\", c=\"0.3\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.title('Calibration plots  (reliability curve)')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Organize data in a structure that can be written to a csv file\n",
    "def getVariablesForPrint(val_cm, test_cm):\n",
    "    test_TP = test_cm[1][1]\n",
    "    test_FN = test_cm[1][0]\n",
    "    test_FP = test_cm[0][1]\n",
    "    test_TN = test_cm[0][0]\n",
    "    val_TP = val_cm[1][1]\n",
    "    val_FN = val_cm[1][0]\n",
    "    val_FP = val_cm[0][1]\n",
    "    val_TN = val_cm[0][0]\n",
    "\n",
    "    val_RE = val_TP / (val_TP + val_FN)\n",
    "    test_RE = test_TP / (test_TP + test_FN)\n",
    "    val_SP = val_TN / (val_TN + val_FP)\n",
    "    test_SP = test_TN / (test_TN + test_FP)\n",
    "\n",
    "    results = {\"val_RE\": val_RE,\n",
    "               \"val_SP\": val_SP,\n",
    "               \"val_TP\": val_TP,\n",
    "               \"val_FP\": val_FP,\n",
    "               \"val_TN\": val_TN,\n",
    "               \"val_FN\": val_FN,\n",
    "               \"test_RE\": test_RE,\n",
    "               \"test_SP\": test_SP,\n",
    "               \"test_TP\": test_TP,\n",
    "               \"test_FP\": test_FP,\n",
    "               \"test_TN\": test_TN,\n",
    "               \"test_FN\": test_FN\n",
    "               }\n",
    "    return results\n",
    "\n",
    "# Apply the given model to data_test\n",
    "def predictValues(data_test, model):\n",
    "    y_pred = model.predict(data_test)\n",
    "    y_prob = model.predict_proba(data_test)\n",
    "\n",
    "    return y_pred, y_prob[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialization of the required parameters. The following are the ones used in the paper\n",
    "params = {\n",
    "  # The number of the last athletes that joined the club used as the test set\n",
    "  \"nTestAthletes\": 25,\n",
    "  \"nbags\": 9,          # The number of XGBoost models\n",
    "  \"calibrationType\": \"sigmoid\",  # The type of calibration - Platt scaling\n",
    "  \"nExp\": 5,          # The number of experiments to run\n",
    "  # The number of injury and non-injury samples taken to train each of the XGBoost models\n",
    "  \"samplesPerClass\": 2048,\n",
    "  # The two approaches (day or week) that we consider in our publication\n",
    "  \"approachList\": [\"day\", \"week\"],\n",
    "  \"XGBEstimatorsList\": [256, 512],\n",
    "  \"XGBDepthList\": [2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = \"day\"\n",
    "outdir = \"%s_%d\" % (approach, params[\"samplesPerClass\"])\n",
    "exp = 1\n",
    "# Loading data and selecting athletes\n",
    "df = loadData(\"day\")\n",
    "athletes = sorted(df['Athlete ID'].unique())\n",
    "# test athletes are last N athletes to join the club\n",
    "test_athletes = athletes[len(athletes) - params['nTestAthletes']:len(athletes)]\n",
    "X_test = df[df[\"Athlete ID\"].isin(test_athletes)]\n",
    "X_test_means, X_test_std = getMeanStd(X_test)\n",
    "\n",
    "# training data is the rest of the athletes\n",
    "X_trainval = df[~df[\"Athlete ID\"].isin(test_athletes)]\n",
    "X_train_means, X_train_std = getMeanStd(X_trainval)\n",
    "\n",
    "x_trainval_org = X_trainval\n",
    "modelList = []\n",
    "featureRanking = pd.DataFrame()\n",
    "aucList = np.zeros(params['nbags'])\n",
    "\n",
    "# Balance and bag the training data\n",
    "X_valorg = getBalancedSubset(x_trainval_org, params['samplesPerClass'])\n",
    "y_val_probList = pd.DataFrame()\n",
    "\n",
    "for bag in np.arange(params['nbags']):\n",
    "  print('bagging data for bag %d' % (bag+1))\n",
    "  X_train_bag = getBalancedSubset(x_trainval_org, params['samplesPerClass'])\n",
    "  model, calib_model, X_val, y_val = trainModel(params, X_train_bag, X_valorg, X_train_means, X_train_std, params['calibrationType'])\n",
    "  modelList.append(calib_model)\n",
    "\n",
    "  y_val_prob = calib_model.predict_proba(X_val)\n",
    "  plotCalibrationCurve(\n",
    "    y_val, y_val_prob[:, 1], './%s/calibrate_%d_%d.pdf' % (outdir, exp, bag))\n",
    "  y_val_probList['bag %d' % (bag+1)] = y_val_prob[:, 1]\n",
    "  featureRanking['bag %d' % (bag+1)] = model.feature_importances_\n",
    "\n",
    "plotCalibrationCurve(\n",
    "  y_val, y_val_prob[:, 1], 'calibration_bag%d.png' % (bag+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
